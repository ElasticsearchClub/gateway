path.data: data
path.logs: log

entry:
  - name: es_gateway #your gateway endpoint
    enabled: true
    router: default
    #configure your gateway's routing flow
#    router: not_found #configure your gateway's routing flow
    network:
      binding: 0.0.0.0:8000
#      skip_occupied_port: false
      reuse_port: true #you can start multi gateway instance, they share same port, to full utilize system's resources
    tls:
      enabled: false #if your es is using https, the gateway entrypoint should enable https too

flow:
  - name: double_write
    filter:
      - name: clone
        parameters:
          flows:
            - write_to_queue_a
            - write_to_queue_b #last one's response will be output to client
  - name: write_to_queue_a
    filter:
      - name: elasticsearch
        parameters:
          elasticsearch: es1
      - name: request_logging
        parameters:
          queue_name: request_logging_a
  - name: write_to_queue_b
    filter:
      - name: elasticsearch
        parameters:
          elasticsearch: es2
      - name: request_logging
        parameters:
          queue_name: request_logging_b

  - name: ingest_to_kafka #ingest to kafka
    filter:
      - name: to_kafka
        parameters:
          topic: 'gateway_bulk_requests'
          group: "default"
          batch_size: 1000
          brokers:
            - localhost:9092
  - name: hello_world #testing flow
    filter:
      - name: echo
        parameters:
          str: "hello infini\n"
          repeat: 1
      - name: echo
        parameters:
          str: "hello gateway\n"
          repeat: 3
  - name: not_found #testing flow
    filter:
      - name: echo
        parameters:
          str: '404 not found\n'
          repeat: 1
  - name: cache_first
    filter: #comment out any filter sections, like you don't need cache or rate-limiter
#      - name: ratio
#        parameters:
#          ratio: 0.5 # 50% of requests will routing to another flow: hello_world.
#          flow: hello_world
#          continue: false # continue the following filters processing after match and executed the flow, set `false` to return ASAP.
      - name: get_cache
#        parameters:
#          pass_patterns: ["_cat","scroll", "scroll_id","_refresh","_cluster","_ccr","_count","_flush","_ilm","_ingest","_license","_migration","_ml","_rollup","_data_stream","_open", "_close"]
#          hash_factor:
#            header:
#              - "*"
#            path: true
#            query_args:
#              - id
#          must_cache:
#            method:
#              - GET
#            path:
#              - _search
#              - _async_search
#      - name: rate_limit
#        parameters:
#          message: "Hey, You just reached our request limit!"
#          rules: #configure match rules against request's PATH, eg: /_cluster/health, match the first rule and return
#            - pattern: "/(?P<index_name>medcl)/_search" #use index name, will match: /medcl/_search, with limit medcl with max_qps ~=3
#              max_qps: 3 #setting max qps after match
#              group: index_name #use regex group name to extract the throttle bucket name
#            - pattern: "/(?P<index_name>.*?)/_search" #use regex pattern to match index, will match any /$index/_search, and limit each index with max_qps ~=100
#              max_qps: 100
#              group: index_name
      - name: elasticsearch
        parameters:
#          elasticsearch: dev  #elasticsearch configure reference name
          elasticsearch: prod  #elasticsearch configure reference name
          max_connection: 1000 #max tcp connection to upstream, default for all nodes
          max_response_size: -1 #default for all nodes
          balancer: weight
          refresh: # refresh upstream nodes list, need to enable this feature to use elasticsearch nodes auto discovery
            enabled: true
            interval: 30s
#          weights: #overwride host's weight, the default weight is 1
#            - host: 192.168.3.201:9200 #the format is host:port
#              weight: 10
#            - host: 192.168.3.202:9200
#              weight: 20
#            - host: 192.168.3.203:9200
#              weight: 30
#          filter:
#            hosts:
#              exclude: #exclude mode, you may need to filter out master nodes
#                - 192.168.3.201:9200
#              include: #specify endpoints to use, in case of that you may only want to forward traffic to coordinating nodes
#                - 192.168.3.202:9200
#                - 192.168.3.203:9200
#            tags:
#              exclude:
#                - temp: cold
#                - temp: hot
#                - temp: warm
#              include:
#                - disk: sd
#                - disk: sata
#                - disk: ssd
#            roles: # "data","ingest","master","ml","remote_cluster_client","transform"
#              exclude:
#                - master
#              include:
#                - data
#                - ingest
      - name: set_cache
#        parameters:
#          min_response_size: 100
#          max_response_size: 1024000
#          cache_ttl: 30s
#          max_cache_items: 100000
  - name: request_logging # this flow is used for request logging, refer to `router`'s `tracing_flow`
    filter:
#      - name: sample
#        parameters:
#          ratio: 0.2
#      - name: request_path_filter
#        parameters:
#          must: #must match all rules to continue
#            prefix:
#              - /medcl
#            contain:
#              - _search
#            suffix:
#              - _count
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
#          must_not: # any match will be filtered
#            prefix:
#              - /.kibana
#              - /_security
#              - /_security
#              - /gateway_requests*
#              - /.reporting
#              - /_monitoring/bulk
#            contain:
#              - _search
#            suffix:
#              - _count
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
#          should:
#            prefix:
#              - /medcl
#            contain:
#              - _search
#              - _async_search
#            suffix:
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
      - name: request_header_filter # filter out the requests that we are not interested, reduce tracing pressure
        parameters:
          exclude: # any rule match will marked request as filtered
          - app: kibana # in order to filter kibana's access log, config `elasticsearch.customHeaders: { "app": "kibana" }` to your kibana's config `/config/kibana.yml`
#          - ENV: test #for example, requests for testing purpose can be filtered, eg: curl localhost:8000 -H "ENV: test"
#          - INFINI-CACHE: CACHED # cached requests can be filtered safely
#          include: # any match will marked as pass through, when include was specified but none of these rules are matched, the request will be mark as filtered, the next filters won't continue processing
#          - TRACE: true # curl localhost:8000 -H "TRACE: true"
#          - TRACING: true # curl localhost:8000 -H "TRACING: true"
#      - name: request_method_filter
#        type: request_method_filter
#        parameters:
#          exclude:
#            - PUT
#            - POST
#          include:
#            - GET
#            - HEAD
#            - DELETE
#      - name: dump_url
#      - name: dump_header
      - name: request_logging
        parameters:
          queue_name: request_logging


router:
  - name: default
    tracing_flow: request_logging #a flow will execute after request finish
    default_flow: cache_first
    rules: #rules can't be conflicted with each other, will be improved in the future
      - id: 1 # this rule means match every requests, and sent to `cache_first` flow
        method:
          - "*"
        pattern:
          - /
#        priority: 1
        flow:
          - cache_first # after match, which processing flow will go through
  - name: not_found
#    tracing_flow: request_logging
#    default_flow: double_write
    default_flow: hello_world
#    default_flow: not_found
#    default_flow: request_logging
#    default_flow: ingest_to_kafka

elasticsearch:
- name: local
  enabled: true
  endpoint: https://127.0.0.1:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
  version: 7.6.0 #optional, used to select es adaptor, can be done automatically after connect to es
  index_prefix: gateway_
  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
    username: elastic
    password: pass
- name: dev
  enabled: true
  endpoint: https://192.168.3.98:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
  index_prefix: gateway_
  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
    username: elastic
    password: pass
- name: prod
  enabled: true
  endpoint: http://192.168.3.201:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
  discovery: # auto discovery elasticsearch cluster nodes
    enabled: true
    refresh:
      enabled: true
#      interval: 10s
  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
    username: elastic
    password: pass
- name: es1
  enabled: true
  version: 7.6.0
  endpoint: http://127.0.0.1:8081  # ./bin/echo_server -port 8081 -debug=true
- name: es2
  enabled: true
  version: 7.6.0
  endpoint: http://127.0.0.1:8082  #./bin/echo_server -port 8082 -debug=true


modules:
- name: elastic
  enabled: true
  elasticsearch: dev
  init_template: true
- name: pipeline
  enabled: true
  runners:
    - name: primary
      enabled: true
      max_go_routine: 1
      threshold_in_ms: 0
      timeout_in_ms: 5000
      pipeline_id: request_logging_index


pipelines:
- name: request_logging_index
  start:
    joint: json_indexing
    enabled: true
    parameters:
      index_name: "gateway_requests"
      elasticsearch: "dev"
      input_queue: "request_logging"
      timeout: "60s"
      worker_size: 1
      bulk_size_in_mb: 1 #in MB
  process: []


queue:
  min_msg_size: 1
  max_msg_size: 5000000000
  max_bytes_per_file: 53687091200
  sync_every_records: 100000 # sync by records count
  sync_timeout_in_ms: 10000 # sync by time in million seconds
  read_chan_buffer: 0

statsd:
  enabled: false
  host: 127.0.0.1
  port: 8125
  namespace: gateway.
