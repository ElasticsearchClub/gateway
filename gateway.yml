path.data: data
path.logs: log

entry:
  - name: es_gateway #your gateway endpoint
    enabled: true
#    router: indexing
    router: default
    #configure your gateway's routing flow
#    router: not_found #configure your gateway's routing flow
    network:
      binding: 0.0.0.0:8000
#      skip_occupied_port: false
      reuse_port: true #you can start multi gateway instance, they share same port, to full utilize system's resources
    tls:
      enabled: true #if your es is using https, the gateway entrypoint should enable https too

flow:
  - name: double_write
    filter:
      - name: clone
        parameters:
          flows:
            - write_to_queue_a
            - write_to_queue_b #last one's response will be output to client
  - name: write_to_queue_a
    filter:
      - name: elasticsearch
        parameters:
          elasticsearch: es1
      - name: request_logging
        parameters:
          queue_name: request_logging_a
  - name: write_to_queue_b
    filter:
      - name: elasticsearch
        parameters:
          elasticsearch: es2
      - name: request_logging
        parameters:
          queue_name: request_logging_b

  - name: ingest_to_kafka #ingest to kafka
    filter:
      - name: to_kafka
        parameters:
          topic: 'gateway_bulk_requests'
          group: "default"
          batch_size: 1000
          brokers:
            - localhost:9092
  - name: hello_world #testing flow
    filter:
      - name: echo
        parameters:
          str: "hello infini\n"
          repeat: 1
      - name: echo
        parameters:
          str: "hello gateway\n"
          repeat: 3
  - name: not_found #testing flow
    filter:
      - name: echo
        parameters:
          str: '404 not found\n'
          repeat: 1
  - name: cache_first
    filter: #comment out any filter sections, like you don't need cache or rate-limiter
#      - name: ratio
#        parameters:
#          ratio: 0.5 # 50% of requests will routing to another flow: hello_world.
#          flow: hello_world
#          continue: false # continue the following filters processing after match and executed the flow, set `false` to return ASAP.
      - name: get_cache
#        parameters:
#          pass_patterns: ["_cat","scroll", "scroll_id","_refresh","_cluster","_ccr","_count","_flush","_ilm","_ingest","_license","_migration","_ml","_rollup","_data_stream","_open", "_close"]
#      - name: rate_limit
#        parameters:
#          message: "Hey, You just reached our request limit!"
#          rules: #configure match rules against request's PATH, eg: /_cluster/health, match the first rule and return
#            - pattern: "/(?P<index_name>medcl)/_search" #use index name, will match: /medcl/_search, with limit medcl with max_qps ~=3
#              max_qps: 3 #setting max qps after match
#              group: index_name #use regex group name to extract the throttle bucket name
#            - pattern: "/(?P<index_name>.*?)/_search" #use regex pattern to match index, will match any /$index/_search, and limit each index with max_qps ~=100
#              max_qps: 100
#              group: index_name
      - name: elasticsearch
        parameters:
#          elasticsearch: dev  #elasticsearch configure reference name
          elasticsearch: prod  #elasticsearch configure reference name
          max_connection: 1000 #max tcp connection to upstream, default for all nodes
          max_response_size: -1 #default for all nodes
          balancer: weight
          refresh: # refresh upstream nodes list, need to enable this feature to use elasticsearch nodes auto discovery
            enabled: true
            interval: 30s
#          weights: #overwride host's weight, the default weight is 1
#            - host: 192.168.3.201:9200 #the format is host:port
#              weight: 10
#            - host: 192.168.3.202:9200
#              weight: 20
#            - host: 192.168.3.203:9200
#              weight: 30
#          filter:
#            hosts:
#              exclude: #exclude mode, you may need to filter out master nodes
#                - 192.168.3.201:9200
#              include: #specify endpoints to use, in case of that you may only want to forward traffic to coordinating nodes
#                - 192.168.3.202:9200
#                - 192.168.3.203:9200
#            tags:
#              exclude:
#                - temp: cold
#                - temp: hot
#                - temp: warm
#              include:
#                - disk: sd
#                - disk: sata
#                - disk: ssd
#            roles: # "data","ingest","master","ml","remote_cluster_client","transform"
#              exclude:
#                - master
#              include:
#                - data
#                - ingest
      - name: set_cache
#        parameters:
#          min_response_size: 100
#          max_response_size: 1024000
#          cache_ttl: 30s
#          max_cached_item: 100000
  - name: request_logging # this flow is used for request logging, refer to `router`'s `tracing_flow`
    filter:
#      - name: sample
#        parameters:
#          ratio: 0.2
#      - name: request_path_filter
#        parameters:
#          must: #must match all rules to continue
#            prefix:
#              - /medcl
#            contain:
#              - _search
#            suffix:
#              - _count
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
#          must_not: # any match will be filtered
#            prefix:
#              - /.kibana
#              - /_security
#              - /_security
#              - /gateway_requests*
#              - /.reporting
#              - /_monitoring/bulk
#            contain:
#              - _search
#            suffix:
#              - _count
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
#          should:
#            prefix:
#              - /medcl
#            contain:
#              - _search
#              - _async_search
#            suffix:
#              - _refresh
#            wildcard:
#              - /*/_refresh
#            regex:
#              - ^/m[\w]+dcl
      - name: request_header_filter # filter out the requests that we are not interested, reduce tracing pressure
        parameters:
          exclude: # any rule match will marked request as filtered
          - app: kibana # in order to filter kibana's access log, config `elasticsearch.customHeaders: { "app": "kibana" }` to your kibana's config `/config/kibana.yml`
#          - ENV: test #for example, requests for testing purpose can be filtered, eg: curl localhost:8000 -H "ENV: test"
#          - INFINI-CACHE: CACHED # cached requests can be filtered safely
#          include: # any match will marked as pass through, when include was specified but none of these rules are matched, the request will be mark as filtered, the next filters won't continue processing
#          - TRACE: true # curl localhost:8000 -H "TRACE: true"
#          - TRACING: true # curl localhost:8000 -H "TRACING: true"
      - name: request_method_filter
        parameters:
          exclude:
            - PUT
            - POST
          include:
            - GET
            - HEAD
            - DELETE
      - name: response_status_filter
        parameters:
          exclude:
            - 200
          include:
            - 404
            - 401
#            - 403
      - name: request_body_truncate
        parameters:
          max_size: 1024
      - name: response_body_truncate
        parameters:
          max_size: 1024
#      - name: dump_request_body
      #      - name: dump_url
      #      - name: dump_header
      - name: request_logging
        parameters:
          queue_name: request_logging
  - name: online_indexing_merge
    filter:
      - name: bulk_reshuffle
        parameters:
          elasticsearch: prod
          level: shard #node or shard
#          level: shard #node or shard
          mode: sync # async or sync
#          mode: async # async or sync
      - name: elasticsearch
        parameters:
          elasticsearch: prod
          refresh:
            enabled: true
            interval: 30s
router:
  - name: default
    tracing_flow: request_logging #a flow will execute after request finish
    default_flow: cache_first
    rules: #rules can't be conflicted with each other, will be improved in the future
      - method:
          - "*"
        pattern:
          - /
#        priority: 1
        flow:
          - cache_first # after match, which processing flow will go through
  - name: not_found
    default_flow: hello_world
  - name: indexing
    default_flow: online_indexing_merge
    tracing_flow: request_logging

elasticsearch:
#- name: local
#  enabled: true
#  endpoint: https://127.0.0.1:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
#  version: 7.6.0 #optional, used to select es adaptor, can be done automatically after connect to es
#  index_prefix: gateway_
#  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
#    username: elastic
#    password: pass
- name: prod
  enabled: true
  endpoint: https://192.168.3.98:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
  index_prefix: gateway_
  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
    username: elastic
    password: pass
  discovery: # auto discovery elasticsearch cluster nodes
    enabled: true
    refresh:
      enabled: true
#- name: prod
#  enabled: true
#  endpoint: http://192.168.3.201:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well
#  discovery: # auto discovery elasticsearch cluster nodes
#    enabled: true
#    refresh:
#      enabled: true
#      module:
#        - nodes
#        - indices
#        - shards
##      interval: 10s
#  basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions
#    username: elastic
#    password: pass
#- name: es1
#  enabled: true
#  version: 7.6.0
#  endpoint: http://127.0.0.1:8081  # ./bin/echo_server -port 8081 -debug=true
#- name: es2
#  enabled: true
#  version: 7.6.0
#  endpoint: http://127.0.0.1:8082  #./bin/echo_server -port 8082 -debug=true


modules:
- name: elastic
  enabled: true
  elasticsearch: prod
  store:
    enabled: true
  orm:
    enabled: true
    init_template: true
    template_name: ".infini-default"
    index_prefix: ".infini-"

- name: pipeline
  enabled: true
  runners:
    - name: primary
      enabled: true
      max_go_routine: 1
      threshold_in_ms: 0
      timeout_in_ms: 5000
      pipeline_id: request_logging_index
    - name: nodes_index
      enabled: true
      max_go_routine: 1
      threshold_in_ms: 0
      timeout_in_ms: 5000
      pipeline_id: bulk_request_ingest


pipelines:
- name: request_logging_index
  start:
    joint: json_indexing
    enabled: true
    parameters:
      index_name: "gateway_requests"
      elasticsearch: "prod"
      input_queue: "request_logging"
      timeout: "60s"
      worker_size: 1
      bulk_size_in_mb: 1 #in MB
- name: bulk_request_ingest
  start:
    joint: bulk_indexing
    enabled: true
    parameters:
      elasticsearch: "prod"
      index:
        - logs-repeat-test
        - logs100million
        - medcl4
      timeout: "60s"
      worker_size: 1
      bulk_size_in_mb: 1 #in MB

floating_ip:
  enabled: true
#  ip: 192.168.3.234      #yep, it's optional, infini-gateway could detect one but maybe not the right one
#  netmask: 255.255.255.0 #optional
#  interface: en1         #optional

queue:
  min_msg_size: 1
  max_msg_size: 5000000000
  max_bytes_per_file: 53687091200
  sync_every_records: 100000 # sync by records count
  sync_timeout_in_ms: 10000 # sync by time in million seconds
  read_chan_buffer: 0

statsd:
  enabled: true
  host: 192.168.3.98
  port: 8125
  namespace: gateway.
