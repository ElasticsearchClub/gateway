path.data: data
path.logs: log

entry:
  - name: my_es_entry
    enabled: true
    router: my_router
    max_concurrency: 10000
#    reduce_memory_usage: true
    network:
      binding: 0.0.0.0:8000
    tls:
      enabled: false

flow:
  - name: primary-read-flow
    filter:
      - if:
          cluster_available: ["primary"]
        then:
          - elasticsearch:
              elasticsearch: "primary"
        else:
          - elasticsearch:
              elasticsearch: "backup"

  - name: primary-flow-post-processing
    filter:
#      - set_basic_auth: #覆盖身份信息
#          username: elastic
#          password: changeme
      - if:
          cluster_available: ["primary"]
        then:
          - retry_limiter:
              queue_name: "primary-deadletter_requests"
              max_retry_times: 60
          - elasticsearch:
              elasticsearch: "primary"
              max_connection_per_node: 1000
          - bulk_response_validate:
              invalid_status: 400
              failure_status: 507
              save_partial_success_requests: true
              partial_success_queue: "primary-partial-success_bulk_requests"
              invalid_queue: "primary-invalid_bulk_requests"
              failure_queue: "primary-failure_bulk_requests"
          - if: #429 重试
              in:
                _ctx.response.status: [ 429 ]
            then:
              - queue:
                  queue_name: "primary-failure"
              - stats:
              - drop:
          - if: #不合法的请求，403,404,413,400，直接返回给客户端，不用入队列
              range:
                _ctx.response.status.gte: 400
                _ctx.response.status.lt: 500
            then:
              - stats:
              - drop:
          - if:
              in:
                _ctx.response.status: [ 200,201 ]
            then:
#              - set_basic_auth: #覆盖身份信息
#                  username: test
#                  password: testtest
              - queue:
                  queue_name: "backup"
            else:
              - queue:
                  queue_name: "primary-failure"
        else:
          - queue:
              queue_name: "primary-failure"
          - elasticsearch_health_check:
              elasticsearch: "primary"
          - sleep:
              sleep_in_million_seconds: 5000
      - flow:
          flows:
            - request_logging
  - name: primary-write-flow
    filter:
#      - set_basic_auth: #覆盖身份信息
#          username: elastic
#          password: changeme
      - if:
          and:
            - queue_has_lag: [ "primary", "primary-failure", "primary-failure_bulk_requests"]
            - cluster_available: ["primary"]
        then: #集群可用但是集群有堆积的情况，不处理客户端请求，待服务恢复之后再提供服务
          - set_response:
              status: 503
              content_type: "application/json"
              body: '{"error":true,"message":"503 Service Unavailable"}'
          - drop:
        else: # 集群不可用或者集群可用且没有堆积的情况，都直接转发给集群先处理
          - if: #集群如果已经变成不可用状态，则直接丢弃请求，让客户端选择处理，或者也可落地队列确保不丢数据
              not:
                cluster_available: ["primary"]
            then: #如果集群不可用，则直接拒绝客户端请求
              - set_response:
                  status: 503
                  content_type: "application/json"
                  body: '{"error":true,"message":"503 Service Unavailable"}'
              - elasticsearch_health_check: #由请求触发的限速模式下的主动检查后端监控情况
                  elasticsearch: "primary"
              - drop:
          - bulk_request_mutate: #修复自动生成 id 的文档，主动生成 id
              fix_null_id: true
              generate_enhanced_id: true
              when:
                contains:
                  _ctx.request.path: /_bulk
          - elasticsearch: #集群可用，直接处理请求
              elasticsearch: "primary"
              max_connection_per_node: 1000
          - bulk_response_validate: #如果是 bulk 请求，还需要进一步验证是否存在部分请求失败的错误异常
              invalid_status: 400
              failure_status: 507
              save_partial_success_requests: true
              partial_success_queue: "primary-partial-success_bulk_requests"
              invalid_queue: "primary-invalid_bulk_requests"
              failure_queue: "primary-failure_bulk_requests"
              when:
                contains:
                  _ctx.request.path: /_bulk
          - if: #429 重试
              in:
                _ctx.response.status: [ 429 ]
            then:
              - queue:
                  queue_name: "primary-failure"
              - drop:
          - if: #不合法的请求，403,404,413,400，直接返回给客户端，不用入队列
              range:
                _ctx.response.status.gte: 400
                _ctx.response.status.lt: 500
            then:
              - drop:
          - if: #正常的请求, 复制到备份集群
              in:
                _ctx.response.status: [200,201]
            then: #仅正常处理的集群才转发给后端集群
#              - set_basic_auth: #覆盖身份信息
#                  username: test
#                  password: testtest
              - queue:
                  queue_name: "backup"
            else: #集群可用的情况下但是失败了，可能存在脏写，将请求放入写入失败队列，后续可以选择两边集群都重做一次，最终确保一致性，写 translog，后续提供 UI 可以进行三方检查：主、备集群和本地日志
              - queue:
                  queue_name: "primary-failure"
  - name: backup-flow-replicate-processing
    filter:
#      - set_basic_auth: #覆盖身份信息
#          username: elastic
#          password: changeme
      - if:
          cluster_available: ["backup"]
        then:
          - retry_limiter:
              queue_name: "backup-deadletter_requests"
              max_retry_times: 60
          - elasticsearch:
              elasticsearch: "backup"
              max_connection_per_node: 1000
          - bulk_response_validate:
              invalid_status: 400
              failure_status: 507
              save_partial_success_requests: false
              invalid_queue: "backup-invalid"
              failure_queue: "backup-failure"
          - if: #429 重试
              in:
                _ctx.response.status: [ 429 ]
            then:
              - queue:
                  queue_name: "backup-failure"
              - stats:
              - drop:
          - if: #不合法的请求，403,404,413,400，直接返回给客户端，不用入队列
              range:
                _ctx.response.status.gte: 400
                _ctx.response.status.lt: 500
            then:
              - stats:
              - queue:
                  queue_name: "backup-invalid"
              - drop:
          - if:
              not:
                in:
                  _ctx.response.status: [ 200,201,404 ]
            then:
              - queue:
                  queue_name: "backup-failure"
        else:
          - queue:
              queue_name: "backup-failure"
          - elasticsearch_health_check:
              elasticsearch: "backup"
          - sleep:
              sleep_in_million_seconds: 5000
      - flow:
          flows:
            - request_logging
  - name: backup-flow-reshuffle-replicate-processing
    filter:
#      - set_basic_auth: #覆盖身份信息
#          username: elastic
#          password: changeme
      - if:
          cluster_available: ["backup"]
        then:
          - retry_limiter:
              queue_name: "backup-deadletter_requests"
              max_retry_times: 60
          - bulk_reshuffle:
              when:
                contains:
                  _ctx.request.path: /_bulk
              elasticsearch: "backup"
              level: node
              mode: async
              fix_null_id: true
          - elasticsearch:
              elasticsearch: "backup"
              max_connection_per_node: 1000
          - bulk_response_validate:
              invalid_status: 400
              failure_status: 507
              save_partial_success_requests: false
              invalid_queue: "backup-invalid"
              failure_queue: "backup-failure"
          - if: #429 重试
              in:
                _ctx.response.status: [ 429 ]
            then:
              - queue:
                  queue_name: "backup-failure"
              - stats:
              - drop:
          - if: #不合法的请求，403,404,413,400，直接返回给客户端，不用入队列
              range:
                _ctx.response.status.gte: 400
                _ctx.response.status.lt: 500
            then:
              - stats:
              - queue:
                  queue_name: "backup-invalid"
#              - drop:
          - if:
              not:
                in:
                  _ctx.response.status: [ 200,201,404 ]
            then:
              - queue:
                  queue_name: "backup-failure"
        else:
          - queue:
              queue_name: "backup-failure"
          - elasticsearch_health_check:
              elasticsearch: "backup"
          - sleep:
              sleep_in_million_seconds: 5000
      - flow:
          flows:
            - request_logging
  - name: request_logging # this flow is used for request logging, refer to `router`'s `tracing_flow`
    filter:
      - stats:
      - logging:
          queue_name: request_logging
          max_request_body_size: 1024
          max_response_body_size: 1024
          when:
            not:
              equals:
                _ctx.response.status: 200

router:
  - name: my_router
    default_flow: primary-write-flow
    tracing_flow: request_logging
    rules:
      - method:
          - "GET"
          - "HEAD"
        pattern:
          - "/{any:*}"
        flow:
          - primary-read-flow
      - method:
          - "POST"
          - "GET"
        pattern:
          - "/_refresh"
          - "/_count"
          - "/_search"
          - "/_msearch"
          - "/_mget"
          - "/{any_index}/_count"
          - "/{any_index}/_search"
          - "/{any_index}/_msearch"
          - "/{any_index}/_mget"
        flow:
          - primary-read-flow

elasticsearch:
  - name: primary
    enabled: true
    endpoint: http://192.168.3.188:9204
    basic_auth:
      username: elastic
      password: changeme
#    traffic_control:
#      max_qps_per_node: 1000
    discovery:
      enabled: true
      refresh:
        enabled: true
        interval: 60s
  - name: backup
    enabled: true
    endpoint: http://192.168.3.188:9206
    basic_auth:
      username: test
      password: testtest
    discovery:
      enabled: true
      refresh:
        enabled: true
        interval: 60s

pipeline:
  - name: request_logging_index
    auto_start: true
    keep_running: true
    processor:
      - json_indexing:
          index_name: "gateway_requests"
          elasticsearch: "backup"
          input_queue: "request_logging"
          when:
            cluster_available: [ "backup" ]

  # pipelines for primary cluster
  - name: queue_primary-dead_retry-consumer
    auto_start: false
    keep_running: false
    processor:
      - flow_runner:
          input_queue: "primary-deadletter_requests"
          flow: primary-flow-post-processing
          when:
            cluster_available: [ "primary" ]
  - name: queue_primary-failure-consumer
    auto_start: true
    keep_running: true
    processor:
      - flow_runner:
          input_queue: "primary-failure"
          flow: primary-flow-post-processing
          when:
            cluster_available: [ "primary" ]
  - name: queue_primary-bulk-failure-consumer
    auto_start: true
    keep_running: true
    processor:
      - flow_runner:
          input_queue: "primary-failure_bulk_requests"
          flow: primary-flow-post-processing
          when:
            cluster_available: [ "primary" ]
  - name: queue_primary-invalid_bulk_requests-consumer
    processor:
      - flow_runner:
          input_queue: "primary-invalid_bulk_requests"
          flow: primary-flow-post-processing
          when:
            cluster_available: [ "primary" ]

## option A - pipelines for backup cluster
#  - name: queue_backup-failure-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - flow_runner:
#          input_queue: "backup-failure"
#          flow: backup-flow-replicate-processing
#          when:
#            cluster_available: [ "backup" ]
#
#  - name: queue_backup-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - flow_runner:
#          input_queue: "backup"
#          flow: backup-flow-replicate-processing
#          when:
#            and:
#              - not:
#                  or:
#                    - queue_has_lag: [ "backup-failure", "primary-partial-success_bulk_requests"]
#              - cluster_available: ["backup"]
#
#  - name: queue_backup-bulk-partial-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - flow_runner:
#          input_queue: "primary-partial-success_bulk_requests"
#          flow: backup-flow-replicate-processing
#          when:
#            cluster_available: [ "backup" ]

## option B - pipelines for backup cluster
#  - name: queue_backup-failure-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - queue_consumer:
#          input_queue: "backup-failure"
#          elasticsearch: "backup"
#          failure_queue: "backup-failure"
#          worker_size: 1
##          compress: true
#          when:
#            cluster_available: [ "backup" ]
#  - name: queue_backup-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - queue_consumer:
#          input_queue: "backup"
#          elasticsearch: "backup"
#          failure_queue: "backup-failure"
#          invalid_queue: "backup-invalid"
#          dead_letter_queue: "backup-dead_letter"
#          waiting_after: [ "primary-partial-success_bulk_requests","backup-failure"]
#          worker_size: 10
##          compress: true
#          when:
#            cluster_available: [ "backup" ]
#  - name: queue_backup-partial-consumer
#    auto_start: true
#    keep_running: true
#    processor:
#      - queue_consumer:
#          input_queue: "primary-partial-success_bulk_requests"
#          elasticsearch: "backup"
#          failure_queue: "backup-failure"
#          invalid_queue: "backup-invalid"
#          dead_letter_queue: "backup-dead_letter"
#          worker_size: 10
##          compress: true
#          when:
#            cluster_available: [ "backup" ]


# option C - pipelines for backup cluster
  - name: queue_backup-failure-consumer
    auto_start: true
    keep_running: true
    processor:
      - flow_runner:
          input_queue: "backup-failure"
          flow: backup-flow-reshuffle-replicate-processing
          when:
            cluster_available: [ "backup" ]

  - name: queue_backup-consumer
    auto_start: true
    keep_running: true
    processor:
      - flow_runner:
          input_queue: "backup"
          flow: backup-flow-reshuffle-replicate-processing
          when:
            and:
              - not:
                  or:
                    - queue_has_lag: [ "backup-failure", "primary-partial-success_bulk_requests"]
              - cluster_available: ["backup"]

  - name: queue_backup-bulk-partial-consumer
    auto_start: true
    keep_running: true
    processor:
      - flow_runner:
          input_queue: "primary-partial-success_bulk_requests"
          flow: backup-flow-reshuffle-replicate-processing
          when:
            cluster_available: [ "backup" ]

  - name: queue_backup-bulk_request_ingest
    auto_start: true
    keep_running: true
    processor:
      - bulk_indexing:
          when:
            cluster_available: [ "backup" ]
          elasticsearch: "backup"
#          compress: true
          bulk_size_in_mb: 10
          worker_size: 10
          level: node
          queues:
            - backup-bulk-failure-items